{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "#### Run this script to proprocess all the data that will be coming in from one, many, or all of the following sources:\n",
    "1. **MonkeyLogic:** (.bhv2 | .h5) behavior data *[500 or 1000Hz]*\n",
    "  > * Photodiode data\n",
    "  > * White Matter Camera Sync analog data (if applicable)\n",
    "  > * White Matter Camera Save analog data (if applicable)\n",
    "2. **SpikeGLX:** (.meta & .bin) duplicates of analog data *[20000-30000Hz]*\n",
    "  > * Photodiode data (split from ML)\n",
    "  > * White Matter Camera Sync analog data (if applicable)\n",
    "  > * White Matter Camera Save analog data (if applicable)\n",
    "3. **WhiteMatter:** (.mp4 | .avi) video files *[60-120 fps]*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Files\n",
    "`FileContainer` will hold the relevant files listed above. <br>\n",
    "* Specify `ROOT_DIR` to set the directory when prompted to select the files.\n",
    "* `WARNING` printed if dates and monkeys not aligned across files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session folder not found.\n",
      "  Select .h5 behavior file (i.e. None_None_choice.h5)\n",
      "Behavior file selected: \n",
      "  MonkeyLogic Date: \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m ROOT_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/rober/Desktop/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     27\u001b[0m VIDEO_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/rober/Desktop/rhAirpuff/videos/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m file_container_obj \u001b[38;5;241m=\u001b[39m \u001b[43mFileContainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVIDEO_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBEHAVIOR_FILE_ONLY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Monkey-Emotions/classes/FileContainer.py:24\u001b[0m, in \u001b[0;36mFileContainer.__init__\u001b[0;34m(self, ROOT_DIR, VIDEO_DIR, MONKEY, DATE, BEHAVIOR_FILE_ONLY)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonkey_name \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVIDEO_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMONKEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBEHAVIOR_FILE_ONLY\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Monkey-Emotions/classes/FileContainer.py:125\u001b[0m, in \u001b[0;36mFileContainer._find_files\u001b[0;34m(self, ROOT_DIR, VIDEO_DIR, MONKEY, DATE, BEHAVIOR_FILE_ONLY)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ml_date\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  MonkeyLogic Date: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ml_date))\n\u001b[0;32m--> 125\u001b[0m ml_monkey_name \u001b[38;5;241m=\u001b[39m \u001b[43mbeh_file_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonkey_name[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ml_monkey_name\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  MonkeyLogic Monkey: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ml_monkey_name))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "# Custom classes\n",
    "from classes.FileContainer import FileContainer\n",
    "from classes.SpikeGLX import SpikeGLX\n",
    "# Custom functions\n",
    "from spike_glx import read_SGLX\n",
    "\n",
    "# pandas options\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "ROOT_DIR = 'C:/Users/rober/Desktop/'\n",
    "VIDEO_DIR = 'C:/Users/rober/Desktop/rhAirpuff/videos/'\n",
    "file_container_obj = FileContainer(ROOT_DIR, VIDEO_DIR, BEHAVIOR_FILE_ONLY=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Convert MonkeyLogic File to Pandas\n",
    "In order to run the behavior files through the preprocessing pipeline, we'll need to convert the MonkeyLogic file (either `.bhv2` or `.h5`) into a Session object which contains the following relevant attributes:\n",
    "> * `session_obj.df`: pandas DataFrame with columns:\n",
    ">   * `'photodiode'`: photodiode signal\n",
    ">   * `'Gen2'`: WM sync signal\n",
    ">   * `'Gen3'`: WM save signal\n",
    "> * `'session_obj.video_path'`: directory of WM videos\n",
    "> * `session_obj.monkey`: monkey name\n",
    "> * `session_obj.date`: date of session (<YYMMDD>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_obj, error_dict, behavioral_code_dict = file_container_obj.ml_to_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_obj.df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from config.h5_helper import pickler\n",
    "\n",
    "dest_path = os.path.join(os.getcwd(), '_data', session_obj.monkey, '_'.join([session_obj.monkey, session_obj.date]))\n",
    "# dest_path = os.path.dirname(file_container_obj.ml_file_path)\n",
    "\n",
    "pickler(True, \n",
    "        save_path=dest_path, \n",
    "        session_df=session_obj.df, \n",
    "        monkey_input='gandalf',\n",
    "        experiment_name='VR',\n",
    "        error_dict=error_dict, \n",
    "        behavioral_code_dict=behavioral_code_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Process SpikeGLX\n",
    "\n",
    "Assign `signal_dict` channels manually based on how you set up the PXI board. `epochs` are the MonkeyLogic eventcodes within `session_obj.df` distinguishing epochs in the task that you will reference later to break up videos based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually assign the signal channel numbers\n",
    "# from how you set them up on the NI PXIe-6341 board\n",
    "signal_dict = {\n",
    "  0: 'cam_sync',\n",
    "  1: 'cam_save',\n",
    "  2: 'lick',\n",
    "  3: 'photodiode',\n",
    "}\n",
    "\n",
    "# Manually assign the time epochs you care about\n",
    "# which have to exist as rows in session_df\n",
    "if session_obj.monkey in ['aragorn', 'bear']:\n",
    "  epochs = ['Start Trial', 'Fixation On', 'CS On',\t\n",
    "          'Trace Start', 'Outcome Start', 'End Trial']\n",
    "else:\n",
    "  epochs = ['Start trial', 'End trial']\n",
    "print('Epochs:')\n",
    "# Print each epoch on its own line\n",
    "for epoch in epochs:\n",
    "  print(f'  {epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek at SpikeGLX Analog Data\n",
    "Always view the data you are working with before doing any type of analysis. Calling `read_SGLX` prompts a file selection tool (choose the .bin file that **must have the .meta file in the same directory as is the default for SpikeGLX output**), and `plot_channels_raw` allows for visualization of a window specified by [tStart - tEnd] **seconds**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_glx.read_SGLX import read_SGLX, plot_channels_raw\n",
    "\n",
    "print('Reading SGLX meta and bin files...')\n",
    "meta, chan_dict = read_SGLX()\n",
    "print(' Complete.')\n",
    "print('Plotting channels...')\n",
    "plot_channels_raw(None, meta, chan_dict, signal_dict, tStart=0, tEnd=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_glx.read_SGLX import plot_analog_ML\n",
    "# epochs = ['Start Trial', 'Fixation On', 'CS On',\t\n",
    "#           'Trace Start', 'Outcome Start', 'End Trial']\n",
    "plot_analog_ML(session_obj.df, epochs, trial_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_glx.load_SGLX import load_sglx\n",
    "\n",
    "# Load the spikeglx object\n",
    "spikeglx_obj = load_sglx(session_obj.df, \n",
    "                         session_obj, \n",
    "                         file_container_obj, \n",
    "                         signal_dict, \n",
    "                         epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-SpikeGLX Session Correlation\n",
    "To see just the correlation matrix for each trial in a session, run `plot_spikeglx_ml_corr` with the newly-generated `spikeglx_obj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_glx.read_SGLX import plot_spikeglx_ml_corr\n",
    "plot_spikeglx_ml_corr(spikeglx_obj.ml_sglx_corr_matrix, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert SpikeGLX Data Into Session DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add spikeglx trial_start times to session_df\n",
    "def add_sglx_trial_times(session_obj, spikeglx_obj):\n",
    "  sglx_trial_sample_start = [spikeglx_obj.trial_times[trial]['start'] for trial in spikeglx_obj.trial_times.keys()]\n",
    "  sglx_trial_sample_end = [spikeglx_obj.trial_times[trial]['end'] for trial in spikeglx_obj.trial_times.keys()]\n",
    "  sglx_start_times = [int(spikeglx_obj.sample_rate*trial_sample/1000) for trial_sample in sglx_trial_sample_start]\n",
    "  sglx_end_times = [int(spikeglx_obj.sample_rate*trial_sample/1000) for trial_sample in sglx_trial_sample_end]\n",
    "  trial_times = [spikeglx_obj.sample_times[sglx_start_time:sglx_end_time] for sglx_start_time, sglx_end_time in zip(sglx_start_times, sglx_end_times)]\n",
    "  session_obj.df['sglx_trial_times'] = trial_times\n",
    "  return session_obj\n",
    "session_obj = add_sglx_trial_times(session_obj, spikeglx_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_obj.df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Neural Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.SpikeContainer import SpikeContainer\n",
    "\n",
    "probe_assignment = {\n",
    "  'imec0': 'PMd',\n",
    "  'imec1': 'HPC',\n",
    "  'imec2': 'DLPFCd',\n",
    "  'imec3': 'DLPFCv'\n",
    "}\n",
    "\n",
    "# set the root directory for the data\n",
    "root='c:/Users/rober/SynologyDrive/Rob'\n",
    "\n",
    "# create a spike container object\n",
    "spike_container = SpikeContainer(\n",
    "  ROOT=root, \n",
    "  session_obj=session_obj,\n",
    "  probes=[1,2,3], \n",
    "  probe_assignment=probe_assignment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_container.cluster_info[spike_container.cluster_info['good'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_neurons = spike_container.cluster_info[spike_container.cluster_info['good'] == True]\n",
    "pickle.dump(good_neurons, open(os.path.join(ROOT_DIR,'good_neurons.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_df = session_obj.df\n",
    "pickle.dump(behavior_df, open(os.path.join(ROOT_DIR,'behavior_df.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_load = pickle.load(open(os.path.join(ROOT_DIR,'behavior_df.pkl'), 'rb'))\n",
    "pickle_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST STEP IS TO PARSE cluster_info.spike_time by session_df.sglx_trial_times\n",
    "good_spike_times = good_neurons['spike_time']\n",
    "# pickle for saving dataframe\n",
    "dest_path = os.path.join(os.getcwd(), '_data', session_obj.monkey, '_'.join([session_obj.monkey, session_obj.date]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Process White Matter Videos\n",
    "\n",
    "After generating a `SpikeGLX` object, you can segment all the White Matter videos by trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video.wm_videos import parse_wm_videos\n",
    "\n",
    "if session_obj.monkey == 'gandalf':\n",
    "  epoch_start = 'start'\n",
    "  epoch_end = 'end'\n",
    "else:\n",
    "  epoch_start = 'Trace Start'\n",
    "  epoch_end = 'Outcome Start'\n",
    "\n",
    "kwargs = {'spikeglx_obj': spikeglx_obj,       # 'spikeglx_obj': spikeglx_obj\n",
    "          'session_obj': session_obj,         # 'session_obj': session_obj\n",
    "          'trial_start': 247,                   # 'trial_start': 0 \n",
    "          'trial_end': len(session_obj.df),   # 'trial_end': len(session_obj.df)\n",
    "          'epoch_start': epoch_start,         # 'epoch_start': 'start'\n",
    "          'epoch_end': epoch_end,             # 'epoch_end': 'end'   \n",
    "          'thread_flag': False,               # 'thread_flag': False\n",
    "          'exclude_camera': ['e3v83c5']}      # 'exclude_camera': ['e3v83c5']        \n",
    "\n",
    "parse_wm_videos(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video.wm_videos import parse_wm_videos\n",
    "\n",
    "if session_obj.monkey == 'gandalf':\n",
    "  epoch_start = 'start'\n",
    "  epoch_end = 'end'\n",
    "else:\n",
    "  epoch_start = 'Trace Start'\n",
    "  epoch_end = 'Outcome Start'\n",
    "\n",
    "kwargs = {'spikeglx_obj': spikeglx_obj,       # 'spikeglx_obj': spikeglx_obj\n",
    "          'session_obj': session_obj,         # 'session_obj': session_obj\n",
    "          'trial_start': 247,                   # 'trial_start': 0 \n",
    "          'trial_end': len(session_obj.df),   # 'trial_end': len(session_obj.df)\n",
    "          'epoch_start': epoch_start,         # 'epoch_start': 'start'\n",
    "          'epoch_end': epoch_end,             # 'epoch_end': 'end'   \n",
    "          'thread_flag': False,               # 'thread_flag': False\n",
    "          'exclude_camera': ['e3v83c5']}      # 'exclude_camera': ['e3v83c5']        \n",
    "\n",
    "parse_wm_videos(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DeepLabCut Pretrained Pose-Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Assign Cameras\n",
    "\n",
    "Based on your WhiteMatter camera setup, assign the identifying code of the camera (e3vXXXX) with what it is pointing to in this dictionary. Depending on whether you set the key have `face` or `body` will drive which DLC pretrained model you end up using (`primate_face` vs. `full_macaque`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "\n",
    "camera_dict = {\n",
    "  'e3v8360':'face_1', \n",
    "  'e3v83d6':'face_2',\n",
    "  'e3v83ad':'body_1',\n",
    "  'e3v831b':'body_2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc_primate.dlc_utils import dlc_config, dlc_downsample\n",
    "video_dir = os.path.join(os.getcwd(), 'video', session_obj.monkey + '_' + session_obj.date)\n",
    "dlc_video_path_dict = dlc_config.get_trial_video_list(video_dir, camera_dict)\n",
    "# dlc_video_path_dict = spikeglx_obj.video_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom modules\n",
    "from dlc_primate.dlc_utils import dlc_config, dlc_downsample\n",
    "\n",
    "# Initialize Project\n",
    "config_path_dict, train_config_path_dict = \\\n",
    "  dlc_config.dlc_initialize_project(dlc_video_path_dict, session_obj, camera_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deeplabcut.refine_labels(config_path_dict['e3v8360'])\n",
    "# deeplabcut.extract_frames(config_path_dict['e3v8360'], 'automatic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Network\n",
    "\n",
    "TRY THIS TODAY\n",
    "https://github.com/DeepLabCut/DeepLabCut/blob/main/deeplabcut/generate_training_dataset/frame_extraction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeplabcut.evaluate_network(config_path_dict['e3v8360'], \n",
    "#                      dlc_video_path_dict['video_path_dict'], \n",
    "#                      plotting=True)\n",
    "# deeplabcut.extract_frames(\n",
    "#           config_path_dict['e3v8360'],\n",
    "#           'automatic',\n",
    "#           'kmeans',\n",
    "#           'GUI',\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DLC\n",
    "dlc_config.dlc_run(config_path_dict, \n",
    "                   dlc_video_path_dict, \n",
    "                   start_video=0, \n",
    "                   end_video=None, \n",
    "                   videotype='mp4', \n",
    "                   create_labeled_video=True,\n",
    "                   session_obj=session_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cam in dlc_video_path_dict.keys():\n",
    "  video_path_list = sorted(dlc_video_path_dict[cam], key=lambda x: int(re.findall(r'(\\d+)_{0}'.format(cam), x)[0]))\n",
    "  config_path = config_path_dict[cam]\n",
    "  deeplabcut.create_labeled_video(\n",
    "\t\t\t\t\tconfig_path, \n",
    "\t\t\t\t\tvideo_path_list, \n",
    "\t\t\t\t\tvideotype='mp4',\n",
    "\t\t\t\t\tdraw_skeleton=True, \n",
    "\t\t\t\t\tfiltered=True,\n",
    "\t\t\t\t\ttrailpoints=5,\n",
    "\t\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Generate MonkeyLogic Behavior Trial Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video.ml_videos import generate_ml_behavior_videos\n",
    "trial_num_selected = 1\n",
    "generate_ml_behavior_videos(session_obj.df, \n",
    "                            session_obj, \n",
    "                            trial_num_selected, \n",
    "                            epoch_start, \n",
    "                            epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facial-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
